{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Данный ноутбук использовал окружение google-colab\n",
        "%pip install catboost fasttext -q"
      ],
      "metadata": {
        "id": "rwlxK5AYASaT",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ],
      "metadata": {
        "id": "3xRUXhCVUzur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ],
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ],
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oINYbn1plrz",
        "outputId": "c3fb50cf-badc-403d-e00b-088bf9779899"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'are': 1,\n",
              " 'brown': 2,\n",
              " 'dog': 3,\n",
              " 'dogs': 4,\n",
              " 'fox': 5,\n",
              " 'foxes': 6,\n",
              " 'jump': 7,\n",
              " 'jumps': 8,\n",
              " 'lazy': 9,\n",
              " 'never': 10,\n",
              " 'over': 11,\n",
              " 'quick': 12,\n",
              " 'quickly': 13,\n",
              " 'the': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ],
      "metadata": {
        "id": "eemkFZ1tVLw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_vectorization(text: str, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[int]:\n",
        "\n",
        "    text_list = normalize_pretokenize_text(text)\n",
        "    result=[]\n",
        "    for word in text_list:\n",
        "      binary_word = np.zeros(len(vocab))\n",
        "      if word in vocab_index:\n",
        "        binary_word[vocab_index[word]]=1\n",
        "        result.append(binary_word)\n",
        "    return result\n",
        "\n",
        "def test_one_hot_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "        print(result)\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        #if len(result) != expected_length:\n",
        "        for binary_word in result:\n",
        "          if len(binary_word)!=expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_one_hot_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "Q2-LJcmbTe04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1fde0d-d23e-4f96-90a0-1d2989c8e125"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]), array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n",
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ],
      "metadata": {
        "id": "hAF8IOYMVT3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    text_list = normalize_pretokenize_text(text)\n",
        "    result={}\n",
        "    for word in text_list:\n",
        "      if word in result:\n",
        "        result[word] += 1\n",
        "      else:\n",
        "        result[word] = 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "        print(result)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_bag_of_words_vectorization()"
      ],
      "metadata": {
        "id": "ScFuXh_9TtJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca211544-d9f0-48d9-b9f4-e605e2ba392e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 2, 'quick': 1, 'brown': 3}\n",
            "Bad-of-Words test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "d6LblWJfX2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_idf(corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None)->Dict[str, float]:\n",
        "  idf_all = {}\n",
        "  for word in vocab:\n",
        "    word_count_all=0\n",
        "    for document in corpus:\n",
        "      doc = normalize_pretokenize_text(document)\n",
        "      word_count_in_doc = doc.count(word)\n",
        "      word_count_all += word_count_in_doc>0\n",
        "\n",
        "    if word_count_all == 0:\n",
        "      idf = 0\n",
        "    else:\n",
        "      idf = np.log(len(corpus) / word_count_all)\n",
        "    idf_all[word] = idf\n",
        "  return idf_all\n",
        "\n",
        "def tf_idf_vectorization_optim(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None, idf_all:Dict[str, float]= None) -> Tuple[List[float], Dict[str, float]]:\n",
        "  text_list = normalize_pretokenize_text(text)\n",
        "  result = []\n",
        "  if idf_all is None:\n",
        "    idf_all = all_idf(corpus, vocab, vocab_index)\n",
        "  for word in vocab:\n",
        "    tf = text_list.count(word)/len(text_list)\n",
        "    idf = idf_all[word]\n",
        "    result.append(tf*idf)\n",
        "  return result, idf_all\n",
        ""
      ],
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_vectorization(text: str, corpus: List[str] = None, vocab: List[str] = None, vocab_index: Dict[str, int] = None) -> List[float]:\n",
        "  text_list = normalize_pretokenize_text(text)\n",
        "  result = []\n",
        "  for word in vocab:\n",
        "    word_count_all=0\n",
        "    tf = text_list.count(word)/len(text_list)\n",
        "    for document in corpus:\n",
        "      doc = normalize_pretokenize_text(document)\n",
        "      word_count_in_doc = doc.count(word)\n",
        "      word_count_all += word_count_in_doc>0\n",
        "    idf = np.log(len(corpus)/word_count_all)\n",
        "    result.append(tf*idf)\n",
        "  return result\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "  try:\n",
        "    text = \"the quick brown\"\n",
        "    result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "    print(result)\n",
        "    if not isinstance(result, list):\n",
        "      return False\n",
        "    expected_length = len(vocab)\n",
        "    if len(result) != expected_length:\n",
        "      return False\n",
        "    for val in result:\n",
        "      if not isinstance(val, float):\n",
        "        return False\n",
        "    print(\"TF-IDF test PASSED\")\n",
        "    return True\n",
        "  except Exception as e:\n",
        "    print(f\"TF-IDF test FAILED: {e}\")\n",
        "    return False"
      ],
      "metadata": {
        "id": "It1sdCe2K5-7"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "GKIyS724T0XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0efe96-bee6-434b-b7c1-65adabfbf46e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[np.float64(0.0), np.float64(0.0), np.float64(0.13515503603605478), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.13515503603605478), np.float64(0.0), np.float64(0.13515503603605478)]\n",
            "TF-IDF test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
      ],
      "metadata": {
        "id": "T0f9FZCrX5_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi_vectorization_optim(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2,\n",
        "    co_matrix: np.ndarray = None,\n",
        "    total_count: int = 0,\n",
        "    individual_count: Dict[int, int] = None\n",
        ") -> List[float]:\n",
        "\n",
        "    rows, cols = co_matrix.shape\n",
        "    ppmi_matrix = np.zeros((rows, cols))\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            p_ij = co_matrix[i, j] / total_count\n",
        "            if p_ij == 0:\n",
        "                continue\n",
        "            p_i = individual_count[i] / total_count\n",
        "            p_j = individual_count[j] / total_count\n",
        "            if p_i > 0 and p_j > 0:\n",
        "                pmi = np.log2(p_ij / (p_i * p_j))\n",
        "                ppmi_matrix[i, j] = max(0, pmi)\n",
        "\n",
        "    text_list = normalize_pretokenize_text(text)\n",
        "    text_vector = np.zeros(len(vocab))\n",
        "    valid_words = 0\n",
        "    for word in text_list:\n",
        "      if word in vocab_index:\n",
        "        idx = vocab_index[word]\n",
        "        text_vector += ppmi_matrix[idx]\n",
        "        valid_words += 1\n",
        "\n",
        "    if valid_words > 0:\n",
        "      text_vector /= valid_words\n",
        "    return text_vector.tolist()"
      ],
      "metadata": {
        "id": "lXFznLAE-J76"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_co_occurrence_matrix(\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> Tuple[np.ndarray, int, Dict[int, int]]:\n",
        "    co_occurrence = np.zeros((len(vocab), len(vocab)))\n",
        "    total_count = 0\n",
        "    individual_count = {}\n",
        "\n",
        "    for document in corpus:\n",
        "      doc_list = normalize_pretokenize_text(document)\n",
        "      for idx, word in enumerate(doc_list):\n",
        "        if word not in vocab_index:\n",
        "          continue\n",
        "        start_window = max(0, idx-window_size)\n",
        "        end_window = min(idx+window_size+1, len(doc_list))\n",
        "        for i in range(start_window, end_window):\n",
        "          if doc_list[i] in vocab_index and doc_list[i]!=word:\n",
        "            co_occurrence[vocab_index[word]][vocab_index[doc_list[i]]] +=1\n",
        "        total_count+=1\n",
        "        if vocab_index[word] in individual_count:\n",
        "          individual_count[vocab_index[word]]+=1\n",
        "        else:\n",
        "          individual_count[vocab_index[word]] = 1\n",
        "\n",
        "    return co_occurrence, total_count, individual_count\n",
        "\n",
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "    co_matrix, total_count, individual_count = get_co_occurrence_matrix(corpus, vocab, vocab_index, window_size)\n",
        "\n",
        "    rows, cols = co_matrix.shape\n",
        "    ppmi_matrix = np.zeros((rows, cols))\n",
        "\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            p_ij = co_matrix[i, j] / total_count\n",
        "            if p_ij == 0:\n",
        "                continue\n",
        "            p_i = individual_count[i] / total_count\n",
        "            p_j = individual_count[j] / total_count\n",
        "            if p_i > 0 and p_j > 0:\n",
        "                pmi = np.log2(p_ij / (p_i * p_j))\n",
        "                ppmi_matrix[i, j] = max(0, pmi)\n",
        "\n",
        "    text_list = normalize_pretokenize_text(text)\n",
        "    text_vector = np.zeros(len(vocab))\n",
        "    for word in text_list:\n",
        "        idx = vocab_index[word]\n",
        "        text_vector += ppmi_matrix[idx]\n",
        "\n",
        "    text_vector /= len(text_list)\n",
        "    return text_vector.tolist()\n",
        "\n",
        "\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "        print(result)\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "HgHmNZy75XFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5da390c7-6957-4529-d69d-c39c2b77211e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.1949875002403854, 1.723308333814104, 2.0566416671474372, 0.0, 1.1949875002403854, 2.3899750004807707, 2.3899750004807707, 0.0, 2.723308333814104, 0.0, 0.0, 1.1949875002403854, 2.0566416671474372, 0.0, 1.3333333333333333]\n",
            "PPMI test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEX8IhaFNjSw",
        "outputId": "ffa43405-1c51-4003-e8b6-ea1015c17008"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'are': 1,\n",
              " 'brown': 2,\n",
              " 'dog': 3,\n",
              " 'dogs': 4,\n",
              " 'fox': 5,\n",
              " 'foxes': 6,\n",
              " 'jump': 7,\n",
              " 'jumps': 8,\n",
              " 'lazy': 9,\n",
              " 'never': 10,\n",
              " 'over': 11,\n",
              " 'quick': 12,\n",
              " 'quickly': 13,\n",
              " 'the': 14}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ],
      "metadata": {
        "id": "FK29va3PBH_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LARKCaxofsne",
        "outputId": "f8a2c626-ae55-4671-f2af-a19e2484fce5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.geeksforgeeks.org/nlp/word-embeddings-using-fasttext/"
      ],
      "metadata": {
        "id": "ECNyg_i5mJVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "\n",
        "def train_fasttext_model(texts, vector_size=100, window_size=2, min_count=1, workers=4):\n",
        "    tokenized_texts = [normalize_pretokenize_text(text) for text in texts]\n",
        "\n",
        "    model = FastText(\n",
        "        sentences=tokenized_texts,\n",
        "        vector_size=vector_size,\n",
        "        window=window_size,\n",
        "        min_count=min_count,\n",
        "        workers=workers,\n",
        "        sg=1,\n",
        "        epochs=10\n",
        "    )\n",
        "\n",
        "    model.save(\"fasttext.model\")\n",
        "    return model\n",
        "def get_fasttext_model(sample_size=50):\n",
        "  dataset = datasets.load_dataset(\"imdb\", split=\"train\").shuffle(seed=42).select(range(sample_size))\n",
        "  train_texts = [item['text'] for item in dataset]\n",
        "  fasttext_model = train_fasttext_model(train_texts)\n",
        "get_fasttext_model(500)"
      ],
      "metadata": {
        "id": "uE20jpv8dz2M"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fasttext_embeddings(\n",
        "    text: str,\n",
        "    model_path: str = \"fasttext.model\"\n",
        ") -> List[np.ndarray]:\n",
        "\n",
        "    try:\n",
        "       model = FastText.load(model_path)\n",
        "    except Exception as e:\n",
        "      return []\n",
        "\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    embeddings = []\n",
        "\n",
        "    for word in words:\n",
        "        embedding = model.wv[word]\n",
        "        embeddings.append(embedding)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "vBcecPpNf4o2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Берт"
      ],
      "metadata": {
        "id": "HWty4gMllci_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "        outputs = model(**inputs)\n",
        "        last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        if pool_method == 'cls':\n",
        "            embeddings = last_hidden_states[:, 0, :]\n",
        "\n",
        "        return embeddings.squeeze(0).detach().numpy()"
      ],
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ],
      "metadata": {
        "id": "E_KoKolrD49R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_dataset(\n",
        "    dataset_name: str = \"imdb\",\n",
        "    vectorizer_type: str = \"bow\",\n",
        "    split: str = \"train\",\n",
        "    sample_size: int = 50,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> Tuple[Any, List, List]:\n",
        "\n",
        "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
        "\n",
        "    if sample_size:\n",
        "        dataset = dataset.shuffle(seed=42).select(range(min(sample_size, len(dataset))))\n",
        "\n",
        "    texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "    labels = [item['label'] for item in dataset if 'label' in item]\n",
        "    if split == \"train\" :\n",
        "        def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "            all_words = []\n",
        "            for text in texts:\n",
        "                words = normalize_pretokenize_text(text)\n",
        "                all_words.extend(words)\n",
        "            vocab = sorted(set(all_words))\n",
        "            vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "            return vocab, vocab_index\n",
        "\n",
        "        vocab, vocab_index = build_vocab(texts)\n",
        "        print(f\"Размерчик чловаря 0_о: {len(vocab)}\")\n",
        "\n",
        "    elif split == \"test\" and vocab is None:\n",
        "        raise ValueError(\"Для теста должен быть передан словарь (0-0)\")\n",
        "\n",
        "    vectorized_data = []\n",
        "    idf_all = None\n",
        "    if vectorizer_type == \"tfidf\":\n",
        "        idf_all = all_idf(texts, vocab, vocab_index)\n",
        "    if vectorizer_type == \"ppmi\":\n",
        "      co_matrix, total_count, individual_count = get_co_occurrence_matrix(texts, vocab, vocab_index)\n",
        "\n",
        "    for text in texts:\n",
        "        if vectorizer_type == \"one_hot\":\n",
        "            word_vectors = one_hot_vectorization(text, vocab, vocab_index)\n",
        "            if word_vectors:\n",
        "                avg_vector = np.mean(word_vectors, axis=0)\n",
        "                vectorized_data.append(avg_vector.tolist())\n",
        "            else:\n",
        "                vectorized_data.append([0] * len(vocab))\n",
        "        elif vectorizer_type == \"bow\":\n",
        "            bow_dict = bag_of_words_vectorization(text)\n",
        "            vector = [bow_dict.get(word, 0) for word in vocab]\n",
        "            vectorized_data.append(vector)\n",
        "        elif vectorizer_type == \"tfidf\":\n",
        "            vector, _= tf_idf_vectorization_optim(text, texts, vocab, vocab_index, idf_all)\n",
        "            vectorized_data.append(vector)\n",
        "        elif vectorizer_type == \"ppmi\":\n",
        "            vectorized_data.append(ppmi_vectorization_optim(text, texts, vocab, vocab_index, 2, co_matrix, total_count, individual_count))\n",
        "        elif vectorizer_type == \"fasttext\":\n",
        "            embeddings = get_fasttext_embeddings(text)\n",
        "            if embeddings:\n",
        "                avg_embedding = np.mean(embeddings, axis=0)\n",
        "                vectorized_data.append(avg_embedding.tolist())\n",
        "            else:\n",
        "                vectorized_data.append([0] * 300)\n",
        "        elif vectorizer_type == \"bert\":\n",
        "            embedding = get_bert_embeddings(text)\n",
        "            vectorized_data.append(embedding.tolist())\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "    return vocab, vectorized_data, labels"
      ],
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "def train(\n",
        "    embeddings_method=\"bow\",\n",
        "    test_size=0.2,\n",
        "    val_size=0.2,\n",
        "    cv_folds=5\n",
        "):\n",
        "    print(f\"\\n{embeddings_method.upper()}\")\n",
        "    print('dataset')\n",
        "    vocab, X_train, y_train = vectorize_dataset(\"imdb\", embeddings_method, \"train\")\n",
        "    _, X_test, y_test = vectorize_dataset(\"imdb\", embeddings_method, \"test\", vocab=vocab, vocab_index={word: idx for idx, word in enumerate(vocab)})\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=50,\n",
        "        learning_rate=0.1,\n",
        "        depth=6,\n",
        "        random_seed=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    print('fit')\n",
        "    model.fit(X_train, y_train, verbose=False)\n",
        "    print('cross')\n",
        "\n",
        "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy')\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "    print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test F1-score: {f1:.4f}\")\n",
        "\n",
        "    return model, accuracy, f1"
      ],
      "metadata": {
        "id": "vwDkL5n8XVMT"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    train(embeddings_method=embeddings_method)"
      ],
      "metadata": {
        "id": "naMqAkjqFHAe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3014d7e1-748e-45eb-b592-68b1e39e4230"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BOW\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.6800 (+/- 0.0800)\n",
            "Test Accuracy: 0.6400\n",
            "Test F1-score: 0.6400\n",
            "\n",
            "ONE_HOT\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.8000 (+/- 0.1265)\n",
            "Test Accuracy: 0.6200\n",
            "Test F1-score: 0.5581\n",
            "\n",
            "TFIDF\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.6400 (+/- 0.0980)\n",
            "Test Accuracy: 0.6400\n",
            "Test F1-score: 0.5909\n",
            "\n",
            "PPMI\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.5800 (+/- 0.1497)\n",
            "Test Accuracy: 0.4800\n",
            "Test F1-score: 0.2353\n",
            "\n",
            "FASTTEXT\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.7000 (+/- 0.1265)\n",
            "Test Accuracy: 0.4800\n",
            "Test F1-score: 0.2353\n",
            "\n",
            "BERT\n",
            "dataset\n",
            "Размерчик чловаря 0_о: 3128\n",
            "fit\n",
            "cross\n",
            "Cross-validation Accuracy: 0.7800 (+/- 0.1960)\n",
            "Test Accuracy: 0.7600\n",
            "Test F1-score: 0.7143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrgE-SoUeUAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
