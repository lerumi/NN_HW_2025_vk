{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        # your code here\n",
        "        return int(logits.argmax(dim=-1))\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        # your code here\n",
        "        probs = logits.softmax(dim=-1)\n",
        "        return int(probs.multinomial(num_samples=1))\n",
        "\n",
        "    def _beam_search_generate(self, prompt: str, max_length: int, num_beams: int) -> str:\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        beam_scores = torch.zeros(num_beams)\n",
        "        beam_sequences = input_ids.repeat(num_beams, 1)\n",
        "\n",
        "        for step in range(max_length - input_ids.size(1)):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(beam_sequences)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            log_probs = F.log_softmax(next_token_logits, dim=-1)\n",
        "            scores = beam_scores.unsqueeze(1) + log_probs\n",
        "\n",
        "\n",
        "            vocab_size = next_token_logits.size(-1)\n",
        "            top_scores, top_indices = torch.topk(scores.view(-1), num_beams)\n",
        "\n",
        "            beam_indices = top_indices // vocab_size\n",
        "            token_indices = top_indices % vocab_size\n",
        "\n",
        "            new_sequences = []\n",
        "            for i in range(num_beams):\n",
        "                beam_idx = beam_indices[i].item()\n",
        "                token_idx = token_indices[i].item()\n",
        "\n",
        "                current_seq = beam_sequences[beam_idx]\n",
        "                current_seq = current_seq.unsqueeze(0)\n",
        "                new_token = torch.tensor([[token_idx]], device=current_seq.device)\n",
        "\n",
        "                new_seq = torch.cat([current_seq, new_token], dim=1)\n",
        "                new_sequences.append(new_seq)\n",
        "\n",
        "            beam_sequences = torch.cat(new_sequences, dim=0)\n",
        "            beam_scores = top_scores\n",
        "\n",
        "\n",
        "            if all(seq[0, -1].item() == self.tokenizer.eos_token_id for seq in new_sequences):\n",
        "                break\n",
        "\n",
        "        best_idx = torch.argmax(beam_scores)\n",
        "        return self.tokenizer.decode(beam_sequences[best_idx], skip_special_tokens=True)\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        # your code here\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "        # your code here\n",
        "        if top_p >= 1.0:\n",
        "            return logits\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[indices_to_remove] = -float('Inf')\n",
        "        return logits\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = 0) -> torch.Tensor:\n",
        "        # your code here\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = -float('Inf')\n",
        "        return logits\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "      # your code here\n",
        "        if strategy == \"beam_search\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        generated = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_length - input_ids.size(1)):\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(generated)\n",
        "                next_token_logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            if strategy == \"greedy\":\n",
        "                next_token = self.greedy_sampling(next_token_logits)\n",
        "            elif strategy == \"random\":\n",
        "                logits_temp = self.apply_temperature(next_token_logits, temperature)\n",
        "                next_token = self.random_sampling(logits_temp)\n",
        "            elif strategy == \"top_k\":\n",
        "                logits_temp = self.apply_temperature(next_token_logits, temperature)\n",
        "                logits_topk = self._apply_top_k(logits_temp, top_k)\n",
        "                next_token = self.random_sampling(logits_topk)\n",
        "            elif strategy == \"top_p\":\n",
        "                logits_temp = self.apply_temperature(next_token_logits, temperature)\n",
        "                logits_topp = self._apply_top_p(logits_temp, top_p)\n",
        "                next_token = self.random_sampling(logits_topp)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "\n",
        "            generated = torch.cat([\n",
        "                generated,\n",
        "                torch.tensor([[next_token]], device=generated.device)\n",
        "            ], dim=1)\n",
        "\n",
        "            if next_token == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.tokenizer.decode(generated[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aNUHC3UmSYd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d23cd45e-9acb-405c-d999-38911f83e303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'A robot walks into a bar and says:'\n",
            "\n",
            "greedy: A robot walks into a bar and says: \"I'm going to buy you a beer.\"\n",
            "\n",
            "random: A robot walks into a bar and says: \"Now, we're going to go fight? You\n",
            "beam: A robot walks into a bar and says: \"I'm going to buy you a beer.\"\n",
            "\n",
            "\n",
            " temperature:\n",
            "temperature 0.3: A robot walks into a bar and says: 'I'm going to be\n",
            "temperature 1.0: A robot walks into a bar and says: \"%s>I want you\n",
            "temperature 1.5: A robot walks into a bar and says: \".Che Seventh inning.\" Saturdays\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = Model(\"gpt2\")\n",
        "prompt = \"A robot walks into a bar and says:\"\n",
        "\n",
        "\n",
        "print(f\"'{prompt}'\\n\")\n",
        "\n",
        "result1 = model.generate(prompt, max_length=20, strategy=\"greedy\")\n",
        "print(f\"greedy: {result1}\")\n",
        "\n",
        "result2 = model.generate(prompt, max_length=20, strategy=\"random\", temperature=0.8)\n",
        "print(f\"random: {result2}\")\n",
        "\n",
        "result3 = model.generate(prompt, max_length=20, strategy=\"beam_search\", num_beams=3)\n",
        "print(f\"beam: {result3}\")\n",
        "\n",
        "print(\"\\n temperature:\")\n",
        "for temp in [0.3, 1.0, 1.5]:\n",
        "    result = model.generate(prompt, max_length=15, strategy=\"random\", temperature=temp)\n",
        "    print(f\"temperature {temp}: {result}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}